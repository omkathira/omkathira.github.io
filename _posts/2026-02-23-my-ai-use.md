---
title: "Ctrl + Shift + T(hink): Using AI to Actually Learn"
date: 2026-02-23
categories: [Blog]
tags: [Neuroscience, Artificial Intelligence, Learning Theory, Research]
math: false
---

This is probably the billionth post of its kind and I’m sure I *haven’t* piqued your interest with that title. It could just be another post on why and how you should use AI for basically everything. But stick with me - because the way most people talk about using AI is that it either cures cancer in a few weeks or becomes the gateway to some kind of postapocalyptic world where the average person can’t do 2 + 2 without a calculator.

![Alt text](/assets/img/posts/dumb-patrick.png)

As an undergrad, I dedicated 4 years of my life studying something you’ve definitely never heard of and something you’ve most definitely heard of by now. My research lies at the intersection of *computational neuroscience* and *machine learning*. In a sentence, I work on understanding what AI models are actually learning and how they learn it - because right now, we mostly just know that they work, but don’t really know why.

But why does any of that matter?

It’s because spending time studying how AI models learn changed the way I think about how I learn - and specifically, how I use AI almost every day.

During my time in college (and even high school), I found myself comfortably using AI tools to get work done, explore a topic, solve a problem I had, or dive deeper into AI itself for my research. I never took a step back to think about what the goal of my work actually was. But over my last semester as a senior, I got a chance to step back and really look at the way I was using AI and connect that back to everything I learned about it.

Now let’s back up a bit. Modern AI models are fundamentally *empirical*. They learn from data and aren't coded with understandable, logical rules. In a way, they implement a fancy version of learning by trial and error. My last few sentences are pretty big oversimplifications (if you’re interested, you can and should read about deep learning theory), but the central idea stands. When an LLM gives you a response that looks incredibly thoughtful and intelligent, it isn’t really telling you something new. LLMs themselves are primarily trained on data that *already exists*. It’s just faster at finding that information and better at curating it than you.

Personally, I do a lot of coding. Not just writing code, but designing and modifying AI models, working through math from research papers, debugging low-level systems code, etc. If this was 2020, I’d get myself a notebook, open up a pretty standard coding tool, and of course, Google. In 2026, I almost always skip the notebook, open up a fancy code editor like Cursor, and (the new *of course* for me) Claude. I also use a lot more specialized AI tools like Claude Code for heavier coding tasks and Devin for codebase indexing.

What *actually* changed between then and now?

In 2020, I’d try tons of approaches to solve a problem. I’d search things up as I needed them, do a deeper search to really learn about something, and start all the way back at square one when something didn’t work, or did, but not very well. That’s similar to learning by trial and error, but the human brain is too complex for me to say I was *only* learning by trial and error. Almost every problem I’ve worked on in my research has multiple solutions. But usually, only one is optimal.

Before we move on, there’s an important distinction between learning and the learning process itself. Learning happens when we internalize information. It could be a concept, a skill (physical or cognitive), an idea, etc. But, if the human brain just relied on memorizing any and all information thrown at it, it’d have to be over a 100 million times larger than it is.

This raises a deeper question - what's actually happening when we get better at solving problems?

Evolution didn’t optimize *what you know*. It optimized *how you learn to know*. This is called *meta-learning* or *learning to learn*. Learning a concept lets you say, “I know this and can use this”. Learning to learn that concept lets you say, “I’m confident enough to learn a similar concept”. My biggest takeaway about AI was precisely this and it's now a principle I follow whenever I use it.

You can and should definitely use AI as a tool to get work done. The human brain is biologically incapable of digesting everything on the internet - but that’s what LLMs are good for. With that in mind, I’ve learned to use AI not as a shortcut to an optimal solution but more as an informed assistant that helps me learn how to come up with a smart set of possible solutions. Instead of asking it to give me the best answer, I ask it to walk me through different approaches and explain the tradeoffs between them. So, the next time I hit a similar problem, I don't need AI anymore.